<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Karma">
    <link rel="stylesheet" href="../highlight/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="../css/stylesheet.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
    <script src="../highlight/highlight.js"></script>
    <script src="../functions.js"></script>
    <script>hljs.highlightAll();</script>
    <script
        src="https://code.jquery.com/jquery-3.3.1.js"
        integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60="
        crossorigin="anonymous">
    </script>
    <script> 
    $(function(){
    $("#header").load("../common/head.html"); 
    });
    </script> 
</head>
<body>
  <!--Add in the header-->
  <div id="header"></div>
<main style = margin-top:100px>
  <nav class="section-nav">
      <ol>
        <li><a href="#common">Common Distributions</a></li>
        <li><a href="#rsamp">Random Sampling</a></li>
        <li><a href="#samp-r">Sampling in R</a>
        </li>
        <li><a href="#dist">Sampling Distributions</a>
          <ol>
            <li><a href="#dist--disc">Discrete Distributions</a></li>
            <li><a href="#dist--cont">Continuous Distributions</a></li>
          </ol>
        </li>
        <li><a href="#clt">Central Limit Theorem</a></li>
        <li><a href="#lln">Law of Large Numbers</a></li>
  
      </ol>
    </nav>
  <div>
    <section id="">
      <h2>Sampling & Sampling Distributions</h2>
      <p>
        Generally, in biology we want to learn something about the world as we define it (eg. all pine trees in North America, Anoles in the Carribean, Drosophila in Africa), but we usually can't measure all individuals in the populations we care about in order to calculate the parameters of interest (e.g. mean weight of all the Anoles, etc). When we can't do that, we need to take a <em>sample</em> of the population, learn something about the sample, and use what we have learned to generalize outward to the population. In fact, if we could sample the entirety of a population of interest, we wouldn't need most types of statistical models (e.g. regression)! We could just calculate our parameters of interest, and be on our way. However, therein the problem lies, since we <em>want</em> usually to make some statement about the whole population, but we don't usually have data on the whole population. So, we take a sample, and try to figure out how best to extrapolate the knowledge we learned from that sample back to the rest of population. 
      </p>
      <p>
        Ideally, when we take some sample of a population it's <em>unbiased</em>, that is, we can take some measurement from the sample such as a mean value, and that value is in fact the same as the population-level value. If we don't collect our sample properly, it's possible that we systematically over or underestimate our population parameter of interest, hence making our estimate <strong>biased</strong>. It's useful to think of some examples of when our sampling protocol might yeild biased samples. Say we want to know the mean size of Anoles in the Carribean. If our sampling method was to do ground observations of Anoles that we could see on a gridded sampling site, it's likely we're more likely to see, and therefore sample, Anoles that are more brightly coloured, and ones that are lower down in trees or on the ground. Is this a problem for our subsequent statistical calculations? Well, only if we know that our sample is biased and a) we don't do anyuthing to change our sampling protocol, and/or b) we don't account for it in our statistical models/tests.
      </p>
      <p>
        Why is this imporant from a statistical perspective? Well, the methods we present in this section usually rely on a <strong>random sample</strong> of the population. Now in reality sometimes a random sample is impossible, and we work with data that have known limitations. An interesting example of this in an observational sample context is camera trap data. There are known biases (the interested reader can refer to <a href="https://conbio.onlinelibrary.wiley.com/doi/pdf/10.1111/csp2.239">this paper on the subject</a>) in camera trap data, but, wildlife researchers who have no choice but to use these data have come up with cleaver methods to account for these biases statistically. These techniques are outside the scope of this manual, but are interesting nonetheless. 
      </p>
      <figure>
        <img src="../img/04-img/trap.jpg" alt="Tiger" style="max-width: 60%; height: auto;">
        <figcaption>A Sumatran tiger captured in this image in Bukit Barisan Selatan National Park in southern Sumatra by wildlife ecologist Max Allen, from the Illinois Natural History Survey.</figcaption>
      </figure> 
      <p>
        Since dealing with biased samples is difficult, we will constrain our discussion of sampling to <strong>random samples</strong>, the premise on which a great many common statistical tests are based. 
      </p>
      <p>
        Statistically, we will think of sampling as referring to the drawing of a subset of data points from some larger set (population). How we sample can be the subject of an entire course, but it is helpful to know enough to perform sampling in R, and then to compile those samples together into a <em>sampling distribution</em>. 
      </p>
    </section>
    <section id="common">
      <h2>Common Distributions</h2>
      <p>
        Before we go any further, let's remind ourselves of the shapes and names of some of the common distributions that we'll want to have fresh in our minds for this section. As another reminder, we need to be conscious as to whether we're dealing with a <em>continuous</em> distribution or a <em>discrete</em> distribution. Here's a visual to remind us of this: 
      </p>
      <figure>
        <img src="../img/04-img/distributions.png">
        <figcaption>credit: Ashkay Sharma</figcaption>
      </figure>
    </section>
    <section id="rsamp">
      <h2>Random Sampling</h2>
      <p>
        So when we want to sample some set of numbers, we have to usually define what kind of distribution we want to draw from. When we ask R to randomly sample a number for us, we need to at least provide it with some guidelines about what kind of number we want. Do we want only integers? Decimals? If so, how many significant digits? What are the limits?
      </p>
      <p>
        The way we would typically think about sampling in our brains is similar to the example of having a set of balls in a bag, all labeled with a unique number from 1-10. Assuming all else about the balls is equal, we could guess that there would be an equal probability we would draw any given ball from the bag. While this feels intuitive, this is actually thinking in distributions! We have actually just stated that we believe the distribution we are sampling from is <strong>uniform</strong>, that is, that all possible values have the same probability of being chosen. 
      </p>
      <p>
        Contrast that with something like a Normal (Gaussian) distribution, and we will be more likely to select a value that lands closer to the center of the distribution, at the mean. 
      </p>
    </section>
    <section id="samp-r">
      <h2>Random Sampling in R</h2>
      <p>
        It's actually quite easy to perform random sampling in R, given that it's a statistical programming language, the basic version of R that comes installed contains the <code>stats</code> package which includes random sampling function for a variety of distributions. Let's use an example with the normal distribution. Since the normal distribution takes two parameters, 1) a mean, and 2) a standard deviation, we need to provide those two parameters to the function, along with the number of samples we'd like to draw. 
      </p>
      <!--begin.rcode
      stats::rnorm(n = 10, mean = 5, sd = 1.5)
      end.rcode-->
      <p>
        And we see R has selected our values for us. Often we want to be able to plot the distribution of values that we've sampled. That's most easily done as a simple histogram, using the <code>hist()</code> function which is in the <code>graphics</code> package: 
      </p>
      <p>
        And we see we've approximated a normal distribution here. 
      </p>
    </section>
        <section id="dist">
      <h2>Sampling Distributions</h2>
      <p>
        A sampling distribution refers to the probability distribution of a particular statistic that we might get from a random sample. 
      </p>
      <section id="dist--disc">
        <h3>Discrete Sampling Distributions</h3>
        <p>
          Let us think back to our example in <a href="./04-01-probability-101.html">Probability 101</a> of a discrete sampling distribution - gene frequency. Imagine we were able to "zoom in" on some random part of our own DNA, we would see something like this: 
        </p>
        <img src="../img/04-img/nucleotide.jpg">
        <p>
          We could imagine that the frequency of each gene may be equivalent, However, research has shown (i.e. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC403801/">Louie et al. (2003)</a>) that nucleotide frequency is <em>not</em> uniform across human genes. So given some particular gene, it may be the case that we see a non-uniform probability distribution, but one that looks like this: 
        </p>
        <!--begin.rcode
        library(ggplot2)
        df <- data.frame(
          base = c("A", "C", "G", "T"),
          prob = c(0.1, 0.2, 0.45, 0.25)
        )

        ggplot(data = df) + 
          geom_col(aes(x = base, y = prob, fill = base)) +
          ggthemes::theme_base() + 
          labs(x = "Nucleotide Base", y = "Probability") + 
          ylim(c(0,1))
        end.rcode-->
        <p>
          As a fun little game, let's build a gene. There approximately 1000 nucleotide pairs of coding sequence per gene according to google, and let's imagine that the gene we're building has a probability distribution of nucleotides that is given above, where the probabilities of each base appearing given a random sample are: 0.1, 0.2, 0.45, and 0.25 respectively for A, C, G, and T. 
        </p>
        <p>
          Using R's functionality, let's sample a variable number times from our weighted set, and see if the distribution we get from our sample matches the theoretical probabilities. We'll sample 20, 100, 1000, and 10,000 times. The Law of Large Numbers indicates that the more samples we take, the closer we'll get to our theoretical probabilities. Let's try!
        </p>
        <p>
          To do this, we'll use R's <code>sample()</code> function from the base package, and alter only the <code>size</code> argument which says how many times to sample.
        </p>
        <!--begin.rcode
        set.seed(1234)
        sample_20 <- sample(c("A", "C", "G", "T"), 
                            size = 20,
                            replace = TRUE,
                            prob = c(0.1, 0.2, 0.45, 0.25))
        sample_100 <- sample(c("A", "C", "G", "T"), 
                            size = 100,
                            replace = TRUE,
                            prob = c(0.1, 0.2, 0.45, 0.25))
        sample_1000 <- sample(c("A", "C", "G", "T"), 
                            size = 1000,
                            replace = TRUE,
                            prob = c(0.1, 0.2, 0.45, 0.25))
        sample_10000 <- sample(c("A", "C", "G", "T"), 
                            size = 10000,
                            replace = TRUE,
                            prob = c(0.1, 0.2, 0.45, 0.25))
        end.rcode-->
        <p>
          Now let's take these vectors and turn them into dataframes so we can plot them. We can use the convenient <code>table()</code> function to get counts of how many of each base there is: 
        </p>
        <!--begin.rcode
        library(dplyr)
        library(ggplot2)
        library(ggthemes)

        df_20 <- data.frame(
          # use table()
          table(sample_20)
          ) %>% 
          # rename them to be consistent
          dplyr::rename(
            base = sample_20,
            prob = Freq
          ) %>% 
          dplyr::rowwise() %>% 
          dplyr::mutate(
            # add column to tell us how many samples were drawn
            sample = "20",
            # turn value from a count to a probability
            prob = prob/20
          )
          
        df_100 <- data.frame(
          # use table()
          table(sample_100)
          ) %>% 
          # rename them to be consistent
          dplyr::rename(
            base = sample_100,
            prob = Freq
          ) %>% 
          dplyr::rowwise() %>% 
          dplyr::mutate(
            # add column to tell us how many samples were drawn
            sample = "100",
            # turn value from a count to a probability
            prob = prob/100
          )

        df_1000 <- data.frame(
          # use table()
          table(sample_1000)
          ) %>% 
          # rename them to be consistent
          dplyr::rename(
            base = sample_1000,
            prob = Freq
          ) %>% 
          dplyr::rowwise() %>% 
          dplyr::mutate(
            # add column to tell us how many samples were drawn
            sample = "1000",
            # turn value from a count to a probability
            prob = prob/1000
          )

        df_10000 <- data.frame(
          # use table()
          table(sample_10000)
          ) %>% 
          # rename them to be consistent
          dplyr::rename(
            base = sample_10000,
            prob = Freq
          ) %>% 
          dplyr::rowwise() %>% 
          dplyr::mutate(
            # add column to tell us how many samples were drawn
            sample = "10000",
            # turn value from a count to a probability
            prob = prob/10000
          )
        end.rcode-->
        <p>
          Now we'll combine all these data frames into one, add a reference of the real values, and plot the result with a grouped bar chart: 
        </p>
        <!--begin.rcode

        # join samples up 
        df_all_samples <- rbind(
          df_20,
          df_100,
          df_1000,
          df_10000
        ) %>% 
          # add in a column to denote these are sampled not theoretical
          dplyr::mutate(
            type = "sampled"
          )

        df_real <- data.frame(
          base = c("A", "C", "G", "T"),
          prob = c(0.1, 0.2, 0.45, 0.25),
          sample = "Real values",
          type = "theoretical"
        )

        # join the dataframes again
        df_all <- rbind(
          df_all_samples %>% 
            dplyr::mutate(
              sample = as.character(sample)
            ),
          df_real
        )

        # plot the result 
        ggplot() + 
          geom_bar(data = df_all, 
                  mapping = aes(x = base, y = prob, alpha = type,
                                fill = sample, colour = type),
                  position = "dodge", stat = "identity") +
          ggthemes::theme_base() +
          scale_fill_manual(
            "Sample Size",
            values = wesanderson::wes_palette("Rushmore1", n = 5)) +
          scale_alpha_manual("Theoretical or Sampled", values = c(0.5, 1)) +
          scale_colour_manual("Theoretical or Sampled", values = c("white", "black"))
          
        end.rcode-->
        <p>
          Here we have shown intuitively the Law of Large Numbers!!!!
        </p>
      </section>
            <section id="dist--cont">
        <h3>Continuous Sampling Distributions</h3>
        <p>
          Consider records for aquatic vertebrates (cutthroat trout and salamanders) in Mack Creek, Andrews Experimental Forest, Oregon (1987 - present). This dataset is present in the <code>lterdatasampler</code> package. 
        </p>
        <!--begin.rcode
        library(lterdatasampler)
        df <- lterdatasampler::and_vertebrates
        head(df)
        end.rcode-->
        <p>
          Let us look at the distribution of cutthroat trout sizes in our database:
        </p>
        <!--begin.rcode
        hist(df$length_1_mm)
        end.rcode-->
        <p>
          We can see that this distribution isn't quite normal, but is continuous, and right-skewed. It could be approximated by a log-normal distribution: 
        </p>
        <!--begin.rcode

        x <- seq(0, 250)
        y <- dlnorm(x, meanlog = 4.1, sdlog = 0.4)

        # figure out how to scale the y for the plot alone 
        scale_factor <- 5000 / 0.01780701
        y <- y * scale_factor
        dist_df <- data.frame(x, y)

        ggplot() + 
          geom_histogram(df, mapping = aes(x = length_1_mm),
                        colour = "grey20", fill = "lightblue") + 
          geom_line(dist_df, mapping = aes(x = x, y = y), 
                    colour = "red", linetype = "dashed", size = 2) +
          ggthemes::theme_base() +
          labs(x = "Cutthroat Trout Length (mm)", y = "Density")
        end.rcode-->
      <p>
        <strong>What will happen if we draw a number of random samples from this distribution and calculate the sample mean? What will the resulting distribution of sample means look like?</strong>
      </p>
      <p>
        What we are doing here is constructing a sampling distribution. We can do this by randomly drawing samples from our population (in this example, every fish in our data set is the "population"), then calculating the mean for each random sample. 
      </p>
      <p>
        As we discuss in the <a href="../07-landingpage.html">programming concepts</a> section on pseudocoding, let's think about how to accomplish this before we start blindly writing code. There are steps to our process here: 
      </p>
      <p>
        <ol>
          <li style="color:blue;font-size:calc(12px + 0.8vw);">1. Determine how many random samples to draw (let's call this <em>n</em>)
          <ol>
            <li style="color:grey;font-size:calc(12px + 0.7vw);">1.1 - How many observations in each sample (let's call this <em>x</em>)? </li>
            <li style="color:grey;font-size:calc(12px + 0.7vw);">1. 2. - Pre-allocate an object to hold <em>n</em> number of sample means </li>
          </ol>
          </li>
          <li style="color:blue;font-size:calc(12px + 0.8vw);">2. Draw a sample 
          <ol>
            <li style="color:grey;font-size:calc(12px + 0.7vw);">2. 1. - Pull out <em>x</em> number of observations </li>
            <li style="color:grey;font-size:calc(12px + 0.7vw);">2. 2. - Calculate the mean </li>
            <li style="color:grey;font-size:calc(12px + 0.7vw);">2. 3. - Store that mean in the corresponding location of the pre-allocated object </li>
          </ol>
          </li>
          <li style="color:blue;font-size:calc(12px + 0.7vw);">3. Repeat step 2 <em>n</em> times</li>
        </ol>
      </p>
      <p>
        The easiest way to actually accomplish this is through a for-loop. We'll start this and do it in order: 
      </p>
      <!--begin.rcode
      # first, we'll draw 100 samples 
      n <- 100

      # let's pull 10 observations in each sample
      x <- 10

      # preallocate object to hold sample means
      sample_means <- vector(mode = "numeric", length = n)
      end.rcode-->
      <p>
        So how would we perform step 2? We'd sample 10 values from <code>df$length_1_mm</code>, calculate the mean, and assign the mean value to <code>sample_means[n]</code>. Then, we simply repeat this <em>n</em> times. Here's that code:
      </p>
      <!--begin.rcode
      # first, sample randomly from the vector of value
      for(i in 1:n) {
        sample_means[i] <- mean(sample(df$length_1_mm, size = x, replace = FALSE))
      }
      head(sample_means)
      end.rcode-->
      <p>
        Okay, so this worked! Excellent. Now, let's plot a histogram of our results:
      </p>
      <!--begin.rcode
      hist(sample_means)
      end.rcode-->
      <p>
        Hmm!! A confusing distribution. This doesn't really resemble any interesting or meaningful distribution. Let's try the process again, but with a much larger value of <em>n</em>, and see if that changes the result: 
      </p>
      <!--begin.rcode
      n <- 100000
      x <- 10
      sample_means <- vector(mode = "numeric", length = n)

      for(i in 1:n) {
        sample_means[i] <- mean(sample(df$length_1_mm, size = x, replace = FALSE))
      }

      hist(sample_means)

      end.rcode-->
      <p>
        Wow! This distribution is remarkably normal! What's going on here????
      </p>
      <p>
        We have just stumbled upon the <strong> Central Limit Theorem</strong>. 
      </p>
    </section>
    </section>
       <section id="clt">
      <h2>Central Limit Theorem</h2>
      <p>
        The Central Limit Theorem is an incredibly important theorem that states the following: 
      </p>
      <p>
        <em>Taking a population with some mean <math>μ</math> and a standard deviation <math>σ</math>, we can sample from the population (with replacement), and with a number of samples that is sufficiently large, the distribution of the sample means will be normally distributed.</em>
      </p>
      <p>
        We have just demonstrated this above with our cutthroat trout example. This is a surprisingly important stated theorem, because it provides us a path forward to work with statistical tests that <strong>assume normal distributions</strong>. As you will learn as you move further through your statistical training, normality of data makes our lives infinitely easier.  
      </p>
      <p>
        I strongly recommend that if you care to learn more about statistics, to think deeply about the central limit theorem, and try to come up your own intuitive understanding of why it is true. There are many excellent descriptions of proofs of the central limit theorem. For the advanced and interested student, there's an excellent <a href="https://www.youtube.com/watch?v=Tx7zzD4aeiA">recorded lecture by John Tsitsiklis at MIT</a> that goes into detail on this theorem and why it is central not only to inferential statistics but how it links statistics to calculus. 
      </p>
    </section>
        <section id="lln">
      <h2>Law of Large Numbers</h2>
      <p>
        In the example we gave on discrete sampling distributions, we have already discovered what this "law" purports. Essentially, it states that if you repeat an experiment some number, <code>n</code>, of times and average the result, as <code>n</code> gets larger, the averaged result will get closer and closer to the expected value.
      </p>
      <p>
        We saw this in our example, as the sampling option with 10,000 samples was the closest one to the expected value. We could deduce a short proof for this, but that's beyond the scope of this short piece.
      </p>
      <p>
        What <em>is</em> an important takeaway is that this law says something valuable to us about what we need to keep in mind when we deal with <strong>small</strong> numbers. That is, the smaller the sample size at hand, the further we are from actually getting a representative sample of the population. While this may seem intuitive, it's helpful to understand that there <em>is</em> a provable way to understand this concept (We can also think about this as it relates to statistical power.)
      </p>
    </section>
<!-- End page content -->
<hr>
<p style="font-size:10px;color:gray;text-align:center">
    <br>
    <br>
    The EEB R Manual is the work of researchers at the University of the Toronto
     and intended as a purely educational resource. It holds no official 
    association with the R Foundation. It should not be taken as an
     authority on R best practices. 
    <br>
    When using this resource, <strong>always</strong> default to instructions and 
    guidance provided by your instructor. 
    <br>
    This content is reviewed regularly for errors and to make improvements, if you see an error and want to help us make this better, see the Contact Page
</p>
</div>
</main>
</body>
</html>
