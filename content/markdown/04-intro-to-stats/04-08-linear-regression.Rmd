---
output:
  pdf_document: default
  html_document: default
---
# Likelihood
**Author:** Cole Brookson
**Date:** 29 August 2022

<em>In a grassland system, is the relationship between species richness and biomass affected by soil pH?</em>

One of the first chapters to a book on causal inference that I have recently read is entitled "All you need is regression". I thought this was an accurate way to start this section, because that statement is true! While we cannot cover all the theory of linear regression here, it is genuinely the case that nearly all classical frequentist statistical methods are simply a version of linear regression. A t-test? Actually just linear regression. ANOVA? Yup, also linear regression. Machine learning?? Many types of it are in fact just regression!

So why then, you may ask, did we learn about t-tests before we learned about linear regression in stats class? Well, it turns out that as with almost all quantitative methods discussed in this section, linear regression is frightfully easy to implement, but actually not so simple "under the hood." 

However, getting an intuitive understanding of the basics is very much possible and highly encouraged. Linear regression is probably the most fundamental of all topics you may learn in statistics, and it's worth thinking about it carefully. To be clear, you will likely re-visit this topic many times, likely over the course of years, adding and refreshing pieces of information that will eventually develop into a solid understanding. here, we attempt only to provide a brief introduction to the basics. 

## What is Regression?

A gross simplification of the process of linear regression would be that if we have a series of points between two continuous variables, we want to find a straight line that goes through as many of those points as possible. 

What does it mean to do such a thing? Well, let's start with reminding ourselves, mathematically, what a line is. A straight line is defined by the equation $$ y = mx + b$$ where $b$ is the y-intercept, $m$ is the slope of the line, and $x$ and $y$ are data values. This means that given a point on the x-axis, $x$, a slope $m$, and a y-intercept $b$, we can find the value $y$ that denotes the y-axis value that corresponds to the x-axis value $x$. 

When we talk about linear regression, we'll probably want to discuss this equation in a slightly alterred form: $$y = \beta_0 + \beta_1 x_1$$. We'll discuss what these terms mean in a moment but they relate exactly to a y-intercept ($\beta_0$), a slope ($\beta_1$) and an x-value. 

## Ordinary Least Squares (OLS)

We mentioned above we're putting a line through some points as best we can. But how do we define this "best as we can"? Well, the version of "as best we can" that will be addressed here is the method of least squares. If we imagine a scatter of plots like this: 

```{r, echo = FALSE, warning = FALSE}
x = c(1:100)
y = rep(NA, 100)
for(i in 1:100) {
  y[i] = 1.3 * x[i] + 10 + rnorm(1, mean = 0, sd = 23)
}

df = data.frame(
  x = x, y = y
)

library(tidyverse)
library(ggthemes)
ggplot(df) + 
  geom_point(aes(x = x, y = y), fill = "red", colour = "black", shape = 21, 
             size = 3) + 
  ggthemes::theme_base()
```

It's clear that we can't actually make our line go <em>directly</em> through very many of them, but perhaps we can figure out some way to make the line <em> as close to as many points as possible</em>. We'll do that by imagining we draw a line at random, and calculate the distance between every single point and the line itself. These are called <strong>residuals</strong>. Then we'll square each one, and take the sum of all those squares. 

You might ask, why square it? The short answer to that question is that squaring it makes a large number of mathematical tools available to us. 

It now follows somewhat logically that we want to find a value for the slope of our line, $\beta_1$ that <strong>minimizes</strong> this sum of the squared residuals. What we are doing here is actually a process called <em>estimation</em> wherein we choose some method that allows us to compare candidate values for $\beta_1$ with other values. Ordinary Least Squares is actually just a specific version of Maximum Likelihood Estimation, which we'll discuss below. 

## Linear Model

As in our previous work, we will certainly NOT (!!!!) just jump into writing code, as that's the best way to ensure a poorly done analysis. First, we have our question. Let's frame it as a hypothesis. It seems logical here to state that we think temperature will positively vary with ice cover. So a null hypothesis may be that <em> there is no relationship between air temperature and ice cover </em>. We have decided we want to use linear regression to tackle this challenge, so consider the assumptions of the method: 

1. Linearity - we assume the relationship between our two variables, $X$ and $Y$ is linear

2. Homoskedacity - we assume the variance of teh residual is the same for any value of $X$

3. Normality - We assume that for any fixed value of $X$, the errors of $Y$ normally distributed

4. Randomness - We also assume that samples were collected at random

5. Independence - All errors are independent

If assumptions 1-3 are violated, then we can may be able to use some sort of transformation on our response variable to deal with the problem. 

### Small example 

Let's now get our hands dirty with a bit of an example, a very small one, that we'll go all the way through and then delve into some details in the next example. Very generally, we can think of the process of doing regression as falling into a few steps: 

1) Inspect our data 

2) Perform the regression

3) Assess our assumptions (post-hoc)

4) Inspect our results

5) Interpret & Visualize


Here are the data we will start with: 

```{r}
# set seed -- see section on this in the probability section
set.seed(1234)

x = c(1:100)
y = rep(NA, 100)
for(i in 1:100) {
  y[i] = 1.3 * x[i] + 10 + rnorm(1, mean = 0, sd = 23)
}

df = data.frame(
  x = x, y = y
)

head(df)
```

Often it's true when doing regression modelling that it's easier to do our diagnostics of assumptions <em>after</em> we actually fit the model itself as some more simple tools will become available to us. So, let's start with fitting the model. As we mentioned before, R makes it dead simple to do a linear regression in R with the <code>stats::lm()</code> function. 

```{r}
mod <- stats::lm(y ~ x, data = df)
```

That's it! Now let's begin seeing if what our model did was "correct". First, we assumed linearly. We can double check this with a simple scatter plot: 

```{r}
library(tidyverse)
library(ggthemes)
ggplot(df) + 
  geom_point(aes(x = x, y = y), fill = "red", colour = "black", shape = 21, 
             size = 3) + 
  ggthemes::theme_base()
```

This looks sufficiently linear for our purposes. However, we can actually check all of our assumptions visually at once by using the diagnostic plots that are created by simply plotting our model object: 

```{r}
par(mfrow = c(2,2)) # this ensures all the plots show up at once
plot(mod)
```

So we have four plots here. 

The first plot is the <strong>Residuals vs. Fitted</strong>. This allows us to check the linear relationship assumption. If the red line approximately follows the grey dashed line and there are no distinct patterns, then the variables are linearly related. 

Second is the <strong>Normal QQ</strong>, and we interpret this the same way we interpreted QQ plots previously. If the points follow the line and don't deviate significantly, then the residuals here are normally distributed. 

Third is the <strong>Spread-Location</strong>, which tests the homogeneity of variance in the residuals (our homoskedasticity). Again, we want to see a horizontal red line with "shotgun" (aka randomly spread) points. 

The last is the <strong>Residuals vs. Leverage</strong>. We don't need to look at this plot too carefully right now, but in brief, it's used to identify points having too strong of an influence on the outcome of the analysis. 

Okay so it looks like all our assumptions are satisfied! Let's take a look at our results: 

```{r}
summary(mod)
```

Let's break down each of the components here: 

1. <strong>Call</strong> -- this is the function we used in our model. So to recall, what we're showing here is the that we've regressed <code>y ~ x</code> which we would say as "y regressed against x" or "y regressed with x". 

2. <strong>Residuals</strong> -- These are the residuals. Recall that a <em>residual</em> is the distance from a single point to the regression line at the same x-location. Here's a visual of that: 

```{r}
ggplot(data = df, mapping = aes(x = x, y = y)) + 
  geom_smooth(method = "lm", formula = y ~ x, colour = "red", se = FALSE) +
  ggthemes::theme_base() +
  geom_point(data = df %>% dplyr::filter(
    x %in% c(22, 45, 89)
  ), size = 3, colour = "black", fill = "purple", shape = 21) +
  geom_line(data = data.frame(x = c(22, 22), y = c(
    predict(mod, data.frame(x = 22)), df$y[which(df$x == 22)]
  )), aes(x = x, y = y)) + 
  geom_line(data = data.frame(x = c(45, 45), y = c(
    predict(mod, data.frame(x = 45)), df$y[which(df$x == 45)]
  )), aes(x = x, y = y)) + 
  geom_line(data = data.frame(x = c(89, 89), y = c(
    predict(mod, data.frame(x = 89)), df$y[which(df$x == 89)]
  )), aes(x = x, y = y)) 
```

Each of the lines here going from the points to the the regression line itself are "residuals", or "residual error", the amount of "error" the linear model produces in it's prediction for a given x-value.

Ok back to our <code>summary()</code> call: 

```{r}
summary(mod)
```

the residuals here, by definition, have a mean of zero, and a "perfecty" minimized set of residuals would mean that the median value is zero, which will not really happen, but we want the median to be as close to zero as possible. We also want our minimum and maximum values to be approximately equal in magnitude. 

3. <strong> Coefficients </strong> -- Possibly the part that we'll look at the most, these values are here shown as representing $\beta_0$ and $\beta_1$. So the intercept is the actual y-intecept (remember that $\beta_0$ is the y-intercept), and the $\beta_1$ is the estimate of the coefficient on $x$. We assess whether or not a particular predictor variable is significantly associated with the outcome variable via the "Pr(>|t|)" statement of the p-value. For our purposes, this will be useful, but I encourage you to interpret these p-values with caution, especially when there are multiple variables. 

For our consideration, we take the <em>Estimate</em> as the actual estimated value for the $\beta$s and then the standard errors (SE), which define accuracy of beta coefficients. A larger SE indicates a less certain beta estimate. We then see the t-statistic and the associated p-value, which speak to the statistical significance of the beta coefficients.

We have to recall that for our regression, we are using the $H_0$ that the estimate is zero, which denotes no effect from said coefficient. If any coefficient has a p-value < 0.05, we can infer that the estimate is significantly different from zero. However, this says nothing about the <em>strength</em> of that interaction. 
 
If we cared to construct a confidence interval around our beta values, we could do so with $\beta +/- 1.96 * SE_{\beta}$, which would give us our 95% CI. 

4. <strong>Model Accuracy</strong> -- We see a number of measures at the bottom, starting with the Residual Standard Error, which contain information about how well our model has fit. 

Starting with the <strong> Residual Standard Error (RSE)</strong>, which essentially represents the average variation of the data points from the fitted regression line. It stands to reason that we want this value to be as small as possible. It comes in handy most when comparing multiple models. Say, for example, we had some other variable $z$ that we think may explain $y$, we could compare the two models $y ~ x$ and $y ~ z$ and see which model has a smaller RSE. This is not the best way to compare models, but it's a good start.

We then see two measures of $R^2$, which is a measure of how well the model fits the data. Specifically, the measure gives the proportion of information (i.e. variation) in the data that can be explained by the model. In general, it's preferred to use the "Adjusted R-squared" as when we add more predictors into our model, the calculation of the Adjusted R-squared will account for that. 

We also see the <em>F-statsitic</em> which is a rough estimate of whether there is a relationship between our predictor and response variables or not. The simple interpretation is that the further the F-statistic is from 1, the stronger the relationship between our predictor and response variables.  

<hr>

Okay so we have all this output. What do we do with it?? Well, that depends on how we've formulated our hypothesis, but let's say that we've stated our hypothesis to be that we think there's a significantly non-zero relationship between x and y. Well we already have our answer in this case! If we look back at our <code>summary()</code> table, then we can see that our t-statistic and p-value are already given. We can see we have a significant result, which says we can reject the null hypothesis that there was no relationship (i.e. $\beta_x = 0$).

### Moving Beyond Simple Regression

So above we had a good example of the fundamentals of linear regression. That involved no transformations of data or anything of the sort. However, recall the first three assumptions of linear regression (linearity, homoskedasticity, and normality): these are very often not satisfied by the type of biological data we're likely to encounter in the "wild". To deal with this problem, we will, from here, abandon the simple linear model. 

## Generalized Linear Models (GLMs)

In a simple linear model, we expect the response variable varies in a linear fashion. That is, some constant amount of change in $X$ leads to a constant change in $Y$. We also need those two variables to be continuous and theoretically unbounded. But this is not always how real data behave!! For example, let's think about species richness. This is a zero-bounded integer value (not continuous!), but we are likely <strong>very</strong> interested in species richness for a variety of questions. Additionally, what about trying to model presence/absence data? Say we wanted to come up with a model for which the response variable is the presence or absence of a gene mutation in a lab rat? These two examples are not well described by a "continuous linear variable". What to do!! We shall simply abandon the linear model :)

The generalized linear model is the more flexible and useful big brother of the simple linear regression. It still uses regression techniques (i.e. fitting a line to some data), and often also uses a version of least squares methods, but adds a bit more complexity to the equation (literally and figuratively!).

These models always contain three things: 

1) A linear predictor 
2) A link function
3) A error distribution

This section is going to add two components - first the link function, and second, linear predictor. In the simple above example, we still had an error structure, but it corresponded to a normal distribution. Our linear predictor was also just a single value. As we move to talking about GLMs, we'll almost always consider multiple predictor variables in our models, as that's what usually makes sense biologically! 

Throughout this section on GLMs, we'll use the following example, which is an excellent example used by JJ Valletta and TJ McKinley in their "Statistical Modelling in R" book: 

A long-term agricultural experiment had 90 grassland plots, each 25m x 25m, differing in biomass, soil pH
and species richness (the count of species in the whole plot). The plots were classified according to a 3-level factor as high,
medium or low pH with 30 plots in each level.

What are We interested in? Well, in many systems, species richness can be affected by biomass, usually negatively. In this grassland, what effect does biomass have on richness? Additionally, if there is a relationship between the two, does it differ across levels of soil pH? Let's set this out in some hypotheses: 

<strong>$H1_0$: Biomass has no effect on species richness. </strong>

<strong>$H2_0$: Differing pH has no effect on the relationship between species richness and biomass. </strong>


Now that we can anchor our thinking with this example, let's go back to the theory for a minute. <strong>I encourage the mathematically timid reader to NOT be afraid of the equations!! We'll explain in detail and maybe you will actually find it fun!!</strong>

In the most basic terms, in a regression of this nature, we have some independent variable(s) $\bf{X}$, and a single dependent variable $\bf{Y}$. For every value of $\bf{X}$, we want to figure out the expected value of $\bf{Y}$. We write that like this: $$\text{E(}\bf{Y}|\bf{X})$$ which we would say as <em>"the expected value of $\bf{Y}$, conditional on $\bf{X}$"</em>

This expected value is then found as: $$\text{E}(\textbf{Y}|\textbf{X}) = \mu =g^{-1}(\textbf{X}\boldsymbol{\beta})$$

The term in the middle, $\mu$, is the mean of the probability distribution. Recall from that section that if $X$ is a random variable, then some expected value $Y$ is therefore going to be the mean of that distribution for that value of $X$. How do we get this mean value? Well, here we set it equal to the value on the righ-hand side.

The term on the right-hand side of the equation has two parts, $\textbf{X}\boldsymbol{\beta}$ is the <strong>linear predictor</strong> and the $g^{-1}()$ is the link function. We'll discuss the linear predictor first. 

### Linear Predictor

This is the value actually tells us how our response variable will change as our predictor variables change. Notice that here we're now talking about multiple predictor variables, as it's nearly always the case that a single variable will not be the only thing we're interested in. Introducing a few equations here will be useful. 

First of all, notice that both terms here ($\textbf{X}\boldsymbol{\beta}$) are bolded. That's because they're both actually vectors, that could contain multiple sets of $\text{X}$ predictor variables, and their associated $\beta$ values. So, to make this more clear, we'll call the linear predictor the Greek letter <em>eta</em>, $\eta$ instead. So we can state $\eta = \textbf{X}\boldsymbol{\beta}$. The exact calculation of $\eta$ is beyond the scope of the present discussion, so just know that when we have a bunch of predictor variables, and their unknown coefficients ($\beta$s), we end up with one single value $\eta$ that is passed to the link function. 

### Link Function 

This is the heart of what makes a model a GLM. This component essentially allows us to relax the assumption of normality of our response variable, by allowing other distributions, and then defining some function of the response variable that in fact DOES vary linearly with the predictor variables. The math behind how this works is both incredibly interesting, and requires some knowledge of linear algebra, so we will leave the interested reader with an encouragement to take a linear algebra class if they have room in their timetable. 

Suffice it to say here that a link function allows us to take some non-linear response data, and "transform" it such that it now varies linearly with the predictor variable(s). 

### Error Distribution

The error distribution we define will in turn define which link function we choose. Determining what error distribution we need to choose can be crudely but often satisfactorily chosen by plotting the response variable, but a more nuanced approach is possible. See <a href="https://r.qcbs.ca/workshop07/book-en/choose-an-error-distribution.html>this tutorial</a> from the Quebec Centre for Biodiversity Science for an example. 

### Species Richness Example

Back to our example stated earlier regarding if the relationship between biomass and species richness changes with soil pH. The data for this example can be found <a href="https://exeter-data-analytics.github.io/StatModelling/_data/species.csv">here</a>. If you want to download the data and follow along, be sure to check out the <a href="../01-basicr/01-06-workflow.html">Workflow section</a>. 

Since we already have our hypothesis stated (and therefore cannot p-hack ourselves), we can proceed to plot our data: 

```{r}
library(tidyverse)


# uncomment and edit the lines below if you need to read in the data 
# library(here)
# df <- readr::read_csv(here("./data/species.csv"))

# quick summary of our data
dplyr::glimpse(df)
head(df)
```

Ok so we can see what types are data are. It's important here to not be tricked by R!! It has read in the data automatically with the <code>Species</code> column as numeric type, but it's in fact integer values. This doens't matter right now, but it will matter when we need to choose an error distribution. 

#### Step 1 - Plot Data
o
```{r}
library(ggthemes)

ggplot(data = df) + 
  geom_point(mapping = aes(x = Biomass, y = Species)) +
  ggthemes::theme_base()
```

Okay, let's try colouring the points by the pH values. First let's check how many levels there are to the variable: 

```{r}
unique(df$pH)
```

Okay, let's plot: 

```{r}
ggplot(data = df) + 
  geom_point(mapping = aes(x = Biomass, y = Species, 
                           fill = pH), colour = "black",
             shape = 21, size = 4) +
  ggthemes::theme_base()
```

Okay, so we see some very very clear groupings here! In fact, it even looks like there's a linear decrease in richness as biomass increases. So why can't we just assume a normal distribution here? Well, let's take a look at it. 

#### An Aside - Motivation for Link Functions

If we simply assume a Normal distribution, we don't even need to specify a link function. I'm going to fit the model and plot the result, with the code behind the scenes so it's super clear this is NOT what you're supposed to do, as a demonstration. 

```{r, echo=FALSE}
# fit model using glm()
fit = glm(Species ~ Biomass*pH,
  data = df,
  family = gaussian(link = "identity"))
df$pH = as.factor(df$pH)
newdata = expand.grid(Biomass = seq(min(df$Biomass), max(df$Biomass),
                                    length.out = 200), pH = levels(df$pH))
newdata_predict = cbind(newdata,

Species = predict(fit, newdata, type = "response"))

# now make our plot
ggplot(data = df, aes(x = Biomass, y =Species, colour = pH)) +
  geom_point() +
  geom_line(data = newdata_predict) + 
  ggthemes::theme_base()
```

Here, the lines are representing the fitted regression lines, with the predicted values of species richness for each value of biomass. What's wrong with this? 

<strong>We're predicting NEGATIVE species richness</strong>. That's impossible! We can probably guess that as biomass gets higher, we'll actually probably have something more like an asymptotic decline of species richness. So how can we model this? <strong>A link function</strong>. 

#### Step 2 - Error Distribution

Let's use our crude by satisfactory method of choosing an error distribution, via plotting our response variable. We can do this a couple ways. As a density plot:

```{r}
ggplot(data = df, mapping = aes(x = Species)) + 
  geom_density(fill = "goldenrod2", alpha = 0.3) +
  ggthemes::theme_base()
  
```

Or perhaps as a histogram: 

```{r}
ggplot(data = df, mapping = aes(x = Species)) + 
  geom_histogram(fill = "blue2", colour = "black", alpha = 0.3) +
  ggthemes::theme_base()
```

What to do? Well, this follows a slightly skewed distribution, but it's discrete integer-valued. For this purpose, a Poisson distribution will work well. 

#### Step 3 - Selecting a Link Function 

We have to come up with a function that will allow us to continue. We can see which link functions are supported for the different distributions in the <code>stats</code> package:

```{r}
?family
```

And a helpful page will pop up. The beginning of the help page has the following info: 

<img src="../img/04-img/family.png">

<strong>And we can see that for the Poisson distribution, we must use the log link function.</strong>

Previously we were interested in $\mu$ to get our expected value of $\text{E(}\bf{Y}|\bf{X})$ but we need to come up with some way to link to $\mu$. It turns out that in this case, our $\textbf{X}\boldsymbol{\beta}$ can be restated as $\textbf{X}\boldsymbol{\beta} = \text{ln}(\mu)$, so in this case, $$\text{E(}\textbf{Y}|\textbf{X}) = \text{exp}(\textbf{X}\boldsymbol{\beta})$$, because we may recall from math class that the exponential function is the natural logarithm. 

#### Step 4 - Fitting the Model 

Okay, let's (Finally!) fit the model. Turns out, yet again, that it's dead simple in R. However, recall that we're interested not just in the effect of pH, but in it's 

```{r}
# first ensure our pH here is a factor

mod_poi <- stats::glm()
```





