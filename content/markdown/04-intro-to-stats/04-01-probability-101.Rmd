---
output:
  pdf_document: default
  html_document: default
---
# Probability 
**Author:** Cole Brookson
**Date:** 29 August 2022

## An Introduction (And Disclaimer...)

There are many excellent texts on probability and probability theory. There are numerous courses in several departments at the University of Toronto, and indeed pretty much any university, that can offer you the student a truly grounded view of what probability is and how to think about it. Indeed, probability itself can be interpreted in multiple ways, and multiple theories about foundational probability and how to interpret it exist. Most of the concepts in this section are unabashedly stolen from the **excellent** (and free!) text on probability by Blitzstein and Hwang which you can find [here](http://probabilitybook.net/). This, however, is a short website page, within which we attempt to discuss some foundational components of probability that we think are important for students in EEB to know. 

There's many reasons why we should study probability, but perhaps most simply, the biological world around us is full of randomness, and even in the processes we think we understand the best, uncertainty is abound. Without an understanding of probability, we don't have a framework with which to confront of understanding or lack thereof, and actually enable a robust discussion about *how much* randomness we actually understand, and how *certain* we are about things we think. 

In this section we'll offer some definitions that will be useful to you in more formally discussing probability. 


**Key Terms:**

1. Random Trials/Experiment
2. Events
3. Outcomes
4. Probability (Naive)
5. Sample Space

## Sample Spaces, Events, and Naive Probability

Probability is based on the mathematical concept of sets, from which we'll steal some jargon. Let's imagine an example to ground this. 

Say there are 1000 spiders in a bag. One of those spiders happens to have a red dot on their thorax. You pull a single spider out of that bag, without looking. You could have either pulled A) a spider with no red dot, or B) the spider with the red dot. What is the *probability* that you have selected the spider with the red dot? 

Above we've actually used all the key terms above in description. First, we've actually defined the *sample space* (the set of all possible outcomes of an experiment/trial) as being finite - the number of spiders in the bag. Sample spaces can be finite, countably infinite, or uncountably infinite. Most biological applications, and *all* the applications we'll consider, have finite sample spaces. That is to say, we can construct a *set* containing every single *outcome* of an *experiment or random trial*. In this example, each spider represents an *outcome*, and an *event* is some set of spiders. **Side note:** sets can have one or even *no* items in them. The *events* are what we would mathematically consider *subsets* of the *sample space*. So here we have two *events* of interest: The subset A) 999 spiders with no red dots, and B) 1 spider with a red dot. 

Here, we have a naive version of a probability, wherein we can actually just define the probability of choosing the spider with the red dot as a simple proportion. To state this clearly, the probability of performing the experiment, and the B) being the outcome can be defined as: 

$$P(\text{B}) = \frac{\text{number of outcomes favourable to B}}{\text{Total number of outcomes in the sample space }\textit{S}}$$
which specifically means: 

$$P(\text{B}) = \frac{\text{number of spiders with red dots}}{\text{total number of spiders in }\textit{S}}$$

Or

$$P(\text{B}) = \frac{1}{1000}$$
$$P(\text{B}) = 0.001$$

Now, to add more conditions, we're assuming here that when we put our hand into the bag to select a spider, there are absolutely **no** defining features that make one spider more or less likely to be chosen. However, there is a case where some spiders are bigger, or perhaps less afraid of the hand coming into the bag, and these traits may make some spiders more or less likely to be chosen. 

This is easy enough to replicate in R, as it's simply a division calculation: 

```{r}
size_b = 1
size_sample_space = 1000
prob_b = size_b / size_sample_space

# show the result
prob_b
```
**SIDE BOX:** Quickly, let's define two (only two!) operations from set theory to help us out. We'll define the sample space *S* as a set, and our two events as subsets *A* and *B* of *S*. We can think of the part of *S* encapsulated by both subset *A* and subset *B* as being the *intersection* of *A* and *B*, and we can separately think about the part of the sample space *S* that is encapsulated by either *A* or *B* as the *union* of *A* and *B*. We write the intersection of *A* and *B* as $A \cap B$ and the union of *A* and *B* as $A \cup B$. Refer to this image for a visual idea of this: 

![](img/sets.png)


### Two Events

For now, let's just consider that there are only two events we're interested in, *A* and *B*. The outcome of *A* is selecting a spider with no red dot, and the outcome of *B* is selecting a spider with a red dot. To make things more interesting, now let's assume there are 234 spiders with red dots (and therefore 766 without red dots). We can visualize this by imagining peeking inside our bag of spiders and seeing something like the following: 

![](img/spiders.png)
If we imagine all the spiders present as the set *S* defining the sample space, it's possible to coarsely replicate the Venn diagram above to make an estimate of what the probabilities of both the union and the intersection might be: 

![spiders](img/spiders-union.png)

We can now see clearly that *all* of the spiders (i.e. all of *S*) is contained within the two event subsets *A* and *B*. Further, there are actually no spiders that fall within both subsets. 

What we have defined here are two *mutually exclusive* events. With only one experiment, the outcome cannot satisfy both events. Formally, we say that the probability of both *A* and *B* is zero: 

$$P(\text{A} \cap \text{B}) = 0$$

But also, since there are are no spiders that fall outside of *A* or *B*, the probability of either one event OR the other occurring with one experiment is actually 1:

$$P(\text{A} \cup \text{B}) = 0$$

### Naive Vs. Standard Probability

The reason to differentiate between these two definitions is one of caution. We defined above the naive probability, which has two incredibly important assumptions: 

1. The sample space is finite

2. Each outcome is equally likely

This clearly serves us well for our current example, and in fact from here on we will refer to the naive probability simply as *probability*, but it's useful to recognize that as soon as we care to make more interesting conditions surrounding our sample space, we must adjust our definition of probability (we won't cover that here).

## Probability Distributions

Our above example was primarily focused on an example where two possible events covered the entire sample space. A commonly used example in the same vein would be flipping a coin - there are only two possibly outcomes: heads or tails. However, this is usually not the case. While it's possible we may only be interested in a handful of the events possible, it's rare that those encompass the entire sample space.

To think about the probability of a variety of events occurring, we can differentiate between *discrete* probabilities and *continuous* probabilities. 

### Discrete Probability Distributions

As the name implies, discrete probability distributions are distributions that describe the probabilities of each possible event in the sample space. For example, let's consider the basic building blocks of DNA, the nucleotide bases adenine (A), cytosine (C), guanine (G) and thymine (T). Imagine we were able to "zoom in" on some random part of our own DNA, we would see something like this: 

![](img/nucleotide.jpg)

If we zoom in on a single base, we can only "see" one of A, C, G, or T. So, our sample space *S* is made up of the four *mutually exclusive* events, which is seeing either A, C, G, or T. If we assume that bases are distributed with even frequency, we can imagine that the probability of each would be the same: 

$$P(\text{A}) = 0.25$$
$$P(\text{C}) = 0.25$$
$$P(\text{G}) = 0.25$$
$$P(\text{T}) = 0.25$$
which would result in a probability distribution that may look like this: 

```{r}
library(ggplot2)
library(ggthemes)

df <- data.frame(
  base = c("A", "C", "G", "T"),
  prob = 0.25
)

ggplot(data = df) + 
  geom_col(aes(x = base, y = prob, fill = base)) +
  ggthemes::theme_base() + 
  labs(x = "Nucleotide Base", y = "Probability") + 
  ylim(c(0,1))
```

This is intuitive. However, research has shown (i.e. [Louie et al. 2003](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC403801/)) that nucleotide frequency is *not* uniform across human genes. So given some particular gene, it may be the case that we see a non-uniform probability distribution, but one that looks like this: 

```{r}
df <- data.frame(
  base = c("A", "C", "G", "T"),
  prob = c(0.1, 0.2, 0.45, 0.25)
)

ggplot(data = df) + 
  geom_col(aes(x = base, y = prob, fill = base)) +
  ggthemes::theme_base() + 
  labs(x = "Nucleotide Base", y = "Probability") + 
  ylim(c(0,1))
```

### Sampling Discrete Probability Distributions

As a fun little game, let's build a gene. There approximately 1000 nucleotide pairs of coding sequence per gene according to google, and let's imagine that the gene we're building has a probability distribution of nucleotides that is given above, where the probabilities of each base appearing given a random sample are: 0.1, 0.2, 0.45, and 0.25 respectively for A, C, G, and T. 

Using R's functionality, let's sample a variable number times from our weighted set, and see if the distribution we get from our sample matches the theoretical probabilities. We'll sample 20, 100, 1000, and 10,000 times. The Law of Large Numbers (LINK TO WHEREVER I END UP PUTTING THIS) indicates that the more samples we take, the closer we'll get to our theoretical probabilities. Let's try!

To do this, we'll use R's `sample()` function from the base package, and alter only the `size` argument which says how many times to sample.

```{r}
set.seed(1234)
sample_20 <- sample(c("A", "C", "G", "T"), 
                    size = 20,
                    replace = TRUE,
                    prob = c(0.1, 0.2, 0.45, 0.25))
sample_100 <- sample(c("A", "C", "G", "T"), 
                    size = 100,
                    replace = TRUE,
                    prob = c(0.1, 0.2, 0.45, 0.25))
sample_1000 <- sample(c("A", "C", "G", "T"), 
                    size = 1000,
                    replace = TRUE,
                    prob = c(0.1, 0.2, 0.45, 0.25))
sample_10000 <- sample(c("A", "C", "G", "T"), 
                    size = 10000,
                    replace = TRUE,
                    prob = c(0.1, 0.2, 0.45, 0.25))
```

Now let's take these vectors and turn them into dataframes so we can plot them. We can use the convenient `table()` function to get counts of how many of each base there is: 

```{r}
library(dplyr)

df_20 <- data.frame(
  # use table()
  table(sample_20)
  ) %>% 
  # rename them to be consistent
  dplyr::rename(
    base = sample_20,
    prob = Freq
  ) %>% 
  dplyr::rowwise() %>% 
  dplyr::mutate(
    # add column to tell us how many samples were drawn
    sample = "20",
    # turn value from a count to a probability
    prob = prob/20
  )
  
df_100 <- data.frame(
  # use table()
  table(sample_100)
  ) %>% 
  # rename them to be consistent
  dplyr::rename(
    base = sample_100,
    prob = Freq
  ) %>% 
  dplyr::rowwise() %>% 
  dplyr::mutate(
    # add column to tell us how many samples were drawn
    sample = "100",
    # turn value from a count to a probability
    prob = prob/100
  )

df_1000 <- data.frame(
  # use table()
  table(sample_1000)
  ) %>% 
  # rename them to be consistent
  dplyr::rename(
    base = sample_1000,
    prob = Freq
  ) %>% 
  dplyr::rowwise() %>% 
  dplyr::mutate(
    # add column to tell us how many samples were drawn
    sample = "1000",
    # turn value from a count to a probability
    prob = prob/1000
  )

df_10000 <- data.frame(
  # use table()
  table(sample_10000)
  ) %>% 
  # rename them to be consistent
  dplyr::rename(
    base = sample_10000,
    prob = Freq
  ) %>% 
  dplyr::rowwise() %>% 
  dplyr::mutate(
    # add column to tell us how many samples were drawn
    sample = "10000",
    # turn value from a count to a probability
    prob = prob/10000
  )
```

Now we'll combine all these data frames into one, add a reference of the real values, and plot the result with a grouped bar chart: 

```{r}
library(ggpattern)

# join samples up 
df_all_samples <- rbind(
  df_20,
  df_100,
  df_1000,
  df_10000
) %>% 
  # add in a column to denote these are sampled not theoretical
  dplyr::mutate(
    type = "sampled"
  )

df_real <- data.frame(
  base = c("A", "C", "G", "T"),
  prob = c(0.1, 0.2, 0.45, 0.25),
  sample = Inf,
  type = "theoretical"
)

# join the dataframes again
df_all <- rbind(
  df_all_samples,
  df_real
)

# plot the result 
ggplot() + 
  geom_bar_pattern(data = df_all_samples, 
           mapping = aes(x = base, y = prob, fill = sample, pattern = type),
           position = "dodge", stat = "identity")
```




















